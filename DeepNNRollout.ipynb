{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-20 23:11:49,969] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ejmejm/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "observation = tflearn.input_data(shape=[None, 4])\n",
    "net = tflearn.fully_connected(observation, 256, activation=\"relu\")\n",
    "net = tflearn.fully_connected(net, 256, activation=\"relu\")\n",
    "net = tflearn.fully_connected(net, 256, activation=\"relu\")\n",
    "out = tflearn.fully_connected(net, 2, activation=\"softmax\")\n",
    "\n",
    "reward_holder = tf.placeholder(tf.float32, [None])\n",
    "action_holder = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "responsible_outputs = tf.gather(tf.reshape(out, [-1]), tf.range(0, tf.shape(out)[0] * tf.shape(out)[1], 2) + action_holder)\n",
    "\n",
    "loss = -tf.reduce_mean(tf.log(responsible_outputs) * reward_holder)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "update = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_reward(rewards):\n",
    "    running_reward = 0\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        rewards[i] = rewards[i] + gamma * running_reward\n",
    "        running_reward += r\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.18\n",
      "26.09\n",
      "31.96\n",
      "52.95\n",
      "86.43\n",
      "129.08\n",
      "145.73\n",
      "191.72\n",
      "194.55\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n",
      "200.0\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1500\n",
    "max_time = 200\n",
    "all_rewards = []\n",
    "saver = tf.train.Saver()\n",
    "train_data = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        ep_history = []\n",
    "        for j in range(max_time):\n",
    "            #Choose an action\n",
    "            a_one_hot = sess.run(out, feed_dict={observation: [obs]}).reshape(2)\n",
    "            action = np.random.choice(a_one_hot, p=a_one_hot)\n",
    "            action = np.argmax(a_one_hot == action)\n",
    "            obs1, r, d, _ = env.step(action)\n",
    "            ep_history.append([obs, r, action])\n",
    "            obs = obs1\n",
    "            episode_reward += r\n",
    "            if d == True:\n",
    "                all_rewards.append(episode_reward)\n",
    "                ep_history = np.array(ep_history)\n",
    "                ep_history[:, 1] = discount_reward(ep_history[:, 1])\n",
    "                train_data.extend(ep_history)\n",
    "                if i % 10 == 0 and i != 0:\n",
    "                    train_data = np.array(train_data)\n",
    "                    sess.run(update, feed_dict={observation: np.vstack(train_data[:, 0]),\n",
    "                                                    reward_holder: train_data[:, 1],\n",
    "                                                    action_holder: train_data[:, 2]})\n",
    "                    train_data = []\n",
    "                break\n",
    "                \n",
    "        if i % 100 == 0 and i != 0:\n",
    "            print(np.mean(all_rewards[-100:]))\n",
    "            \n",
    "    saver.save(sess, \"/tmp/model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_reward = [np.mean(all_rewards[i-10:i+10]) for i in range(10, len(all_rewards))]\n",
    "sns.plt.plot(avg_reward[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_time = 200\n",
    "saver = tf.train.Saver()\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"/tmp/model.ckpt\")\n",
    "    #Show the results\n",
    "    for i in range(10):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        for j in range(max_time):\n",
    "            #Choose an action\n",
    "            a_one_hot = sess.run(out, feed_dict={observation: [obs]}).reshape(2)\n",
    "            action = np.random.choice(a_one_hot, p=a_one_hot)\n",
    "            action = np.argmax(a_one_hot == action)\n",
    "            env.render()\n",
    "            time.sleep(0.005)\n",
    "            obs, r, d, _ = env.step(action)\n",
    "            episode_reward += r\n",
    "            if d == True:\n",
    "                break\n",
    "        print(episode_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
